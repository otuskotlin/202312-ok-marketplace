# Список моделей машинного обучения в 2023 году

| Model                    | Best For                                                 | Main Contributor/Author               | Languages Supported                                                                                                                                                                                                                                                                     | Versions                               | Input Parameters        | Min GPU for Inference  | Min GPU for Learning   | Smartness     | License                  | Significant Restrictions                | Web Link                                                                                           |
|--------------------------|----------------------------------------------------------|---------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------|-------------------------|------------------------|-------------------------|---------------|--------------------------|-----------------------------------------|---------------------------------------------------------------------------------------------------|
| **LLaMA**                | General-purpose NLP tasks                                | Meta AI, USA                          | Multiple languages                                                                                                                                                                                                                                                                      | LLaMA-7B                               | 7 billion               | 16 GB VRAM             | 32 GB VRAM              | High          | Custom                   | Non-commercial academic use only        | [LLaMA on GitHub](https://github.com/facebookresearch/llama)                                      |
|                          |                                                          |                                       |                                                                                                                                                                                                                                                                                           | LLaMA-13B                              | 13 billion              | 24 GB VRAM             | 48 GB VRAM              | Very High     |                          |                                         |                                                                                                   |
| **mT5**                  | Text-to-text transformation tasks                        | Google Research, USA                  | Over 100 languages including English, Spanish, French, Chinese, and many more                                                                                                                                                                                                           | mT5-Base                               | 580 million             | 8 GB VRAM              | 16 GB VRAM              | High          | Apache 2.0               | Few restrictions, permissive license    | [mT5 on Hugging Face](https://huggingface.co/google/mt5-base)                                     |
|                          |                                                          |                                       |                                                                                                                                                                                                                                                                                           | mT5-Large                              | 1.2 billion             | 16 GB VRAM             | 32 GB VRAM              | Very High     |                          |                                         |                                                                                                   |
|                          |                                                          |                                       |                                                                                                                                                                                                                                                                                           | mT5-XL                                 | 3.7 billion             | 24 GB VRAM             | 48 GB VRAM              | Very High     |                          |                                         |                                                                                                   |
| **XLM-R**                | Multilingual understanding tasks                         | Facebook AI Research (FAIR), USA      | 100 languages including major languages like English, Chinese, Spanish, Arabic, etc.                                                                                                                                                                                                    | XLM-R Base                             | 270 million             | 8 GB VRAM              | 16 GB VRAM              | High          | MIT                      | Very few restrictions, highly permissive | [XLM-R on Hugging Face](https://huggingface.co/xlm-roberta-large)                                 |
|                          |                                                          |                                       |                                                                                                                                                                                                                                                                                           | XLM-R Large                            | 550 million             | 16 GB VRAM             | 32 GB VRAM              | Very High     |                          |                                         |                                                                                                   |
|                          |                                                          |                                       |                                                                                                                                                                                                                                                                                           | XLM-R-XXL                              | 3.5 billion             | 24 GB VRAM             | 48 GB VRAM              | Very High     |                          |                                         |                                                                                                   |
| **GPT-NeoX**             | Text generation and language understanding tasks         | EleutherAI, USA                       | Primarily trained on English data, but adaptable to other languages with fine-tuning                                                                                                                                                                                                    | GPT-NeoX-6B                            | 6 billion               | 24 GB VRAM             | 48 GB VRAM              | Very High     | Apache 2.0               | Few restrictions, permissive license    | [GPT-NeoX on GitHub](https://github.com/EleutherAI/gpt-neox)                                      |
| **BERT-multilingual**    | Text classification, named entity recognition, QA        | Google Research, USA                  | 104 languages including English, Chinese, French, Spanish, Arabic, Hindi, etc.                                                                                                                                                                                                          | BERT-Base Multilingual Cased           | 110 million             | 8 GB VRAM              | 16 GB VRAM              | High          | Apache 2.0               | Few restrictions, permissive license    | [BERT-multilingual on Hugging Face](https://huggingface.co/bert-base-multilingual-cased)           |
|                          |                                                          |                                       |                                                                                                                                                                                                                                                                                           | BERT-Large Multilingual Cased (unofficial) | 340 million             | 16 GB VRAM             | 32 GB VRAM              | Very High     |                          |                                         |                                                                                                   |
| **Chinese-BERT-wwm**     | Chinese NLP tasks                                        | Chinese NLP Group, Harbin Institute of Technology, China | Primarily Chinese                                                                                                                                                                                                                                                                       | Chinese-BERT-wwm                       | 110 million             | 8 GB VRAM              | 16 GB VRAM              | High          | Apache 2.0               | Few restrictions, permissive license    | [Chinese-BERT-wwm on GitHub](https://github.com/ymcui/Chinese-BERT-wwm)                           |
|                          |                                                          |                                       |                                                                                                                                                                                                                                                                                           | Chinese-BERT-wwm-ext                   | 110 million             | 8 GB VRAM              | 16 GB VRAM              | High          |                          |                                         |                                                                                                   |
| **MuRIL**                | NLP tasks specific to Indian languages                   | Google Research India, India          | 17 Indian languages including Hindi, Bengali, Tamil, Telugu, Marathi, and more                                                                                                                                                                                                          | MuRIL Base                             | 110 million             | 8 GB VRAM              | 16 GB VRAM              | High          | Apache 2.0               | Few restrictions, permissive license    | [MuRIL on Hugging Face](https://huggingface.co/google/muril-base-cased)                           |
|                          |                                                          |                                       |                                                                                                                                                                                                                                                                                           | MuRIL Large                            | 340 million             | 16 GB VRAM             | 32 GB VRAM              | Very High     |                          |                                         |                                                                                                   |
| **AlephBERT**            | Hebrew NLP tasks                                         | AlephBERT team, Israel                | Primarily Hebrew                                                                                                                                                                                                                                                                        | AlephBERT-Base                         | 110 million             | 8 GB VRAM              | 16 GB VRAM              | High          | Apache 2.0               | Few restrictions, permissive license    | [AlephBERT on Hugging Face](https://huggingface.co/avichr/AlephBERTgimmel-Base)                    |
| **ruBERT**               | Russian NLP tasks                                        | SberAI, Russia                        | Primarily Russian                                                                                                                                                                                                                                                                       | ruBERT                                 | 110 million             | 8 GB VRAM              | 16 GB VRAM              | High          | Apache 2.0               | Few restrictions, permissive license    | [ruBERT on Hugging Face](https://huggingface.co/DeepPavlov/rubert-base-cased)                      |
| **ruGPT-3**              | Text generation and language understanding in Russian    | SberAI, Russia                        | Primarily Russian                                                                                                                                                                                                                                                                       | ruGPT-3 Small                          | 760 million             | 12 GB VRAM             | 24 GB VRAM              | High          | Apache 2.0               | Few restrictions, permissive license    | [ruGPT-3 on GitHub](https://github.com/sberbank-ai/ru-gpts)                                       |
|                          |                                                          |                                       |                                                                                                                                                                                                                                                                                           | ruGPT-3 Medium                         | 1.3 billion             | 16 GB VRAM             | 32 GB VRAM              | High          |                          |                                         |                                                                                                   |
|                          |                                                          |                                       |                                                                                                                                                                                                                                                                                           | ruGPT-3 Large                          | 2.6 billion             | 24 GB VRAM             | 48 GB VRAM              | Very High     |                          |                                         |                                                                                                   |

This table should provide a comprehensive overview of various open-source, multilingual language models, including those from non-US contributors.
